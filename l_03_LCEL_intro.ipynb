{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦œðŸ”— LCEL introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LCEL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "- LangChain Expression Language, or LCEL, is a declarative way to compose chains easily. <br>\n",
    "- This is a new sintaxe for creating chains and agents.\n",
    "</h4>\n",
    "\n",
    "<h3>Pipe Syntax</h3>\n",
    "\n",
    "Composition (chains) can now use the Linux pipe syntax.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "chain = prompt | llm | OutputParser\n",
    "```\n",
    "\n",
    "<h3>Interface</h3>\n",
    "\n",
    "- Components implement the <u>Runnable</u> protocol.\n",
    "- Common methods include invoke, stream, and batch.\n",
    "\n",
    "<h3>Common I/O</h3>\n",
    "\n",
    "|Component     |Input Type                                  | Output Type         |\n",
    "|---           |---                                         |---                  |\n",
    "|Prompt        |Dictionary                                  |Prompt Value         |\n",
    "|Retriever     |Single String                               |List of Documents    |\n",
    "|LLM           |String, list of messages, or a prompt value |String               |\n",
    "|ChatModel     |String, list of messages, or a prompt value |ChatMessage          |\n",
    "|Tool          |String or Dictionary                        |Tool dependent       |\n",
    "|OutputParser  |Output of LLM or ChatModel                  |Parser dependent     |\n",
    "\n",
    "<h3>Runnable Support</h3>\n",
    "\n",
    "- Async, Batch, and streaming support out of the box.\n",
    "- Fallbacks.\n",
    "- Parallelism.\n",
    "  - LLM calls can be time-consuming\n",
    "  - Any components that can be run in parallel are.\n",
    "- Logging is built in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h1>LCEL Introduction</h1>\n",
       "âœ… Libraries loaded successfully <br>\n",
       "âœ… OpenAI Key assigned (<font color=\"LightGreen\">...crmJ6aYp5IUQiT3BlbkFJMcylU...</font>)\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from util import local_settings\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<h1>LCEL Introduction</h1>\n",
    "âœ… Libraries loaded successfully <br>\n",
    "âœ… OpenAI Key assigned (<font color=\"LightGreen\">...{local_settings.OPENAI_API_KEY[10:-15]}...</font>)\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§‘â€ðŸ³ Example 1: Simple chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt, Model, and Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"tell me a short joke about {topic}. After generate a joke, please also translate to portuguese\"\"\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the chicken go to the seance? To talk to the other side! \n",
      "\n",
      "Por que a galinha foi Ã  sessÃ£o espÃ­rita? Para falar com o outro lado!\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"topic\" : \"Chicken\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§‘â€ðŸ³ Example 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green', 'yellow', 'purple']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "\n",
    "result = chain.invoke({\"text\": \"colors\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x11955d330>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "texts =  [\n",
    "    \"Harrison worked at kensho\",\n",
    "    \"bears like to eat honey\",\n",
    "    \"Each day is a new opportunity for growth and success.\",\n",
    "    \"I believe in my abilities and trust the journey I am on.\",\n",
    "    \"I radiate positivity and attract abundance into my life.\",\n",
    "    \"I am deserving of love, happiness, and all the good things life has to offer.\",\n",
    "]\n",
    "\n",
    "retriever = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Harrison worked at kensho'),\n",
       " Document(page_content='I believe in my abilities and trust the journey I am on.'),\n",
       " Document(page_content='bears like to eat honey'),\n",
       " Document(page_content='Each day is a new opportunity for growth and success.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever.similarity_search(\"Where did Harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Where did Harrison work?\"\n",
    "\n",
    "docs =  retriever.similarity_search(prompt)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
